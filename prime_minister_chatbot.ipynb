{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install openai==0.28.1\n",
        "!pip install chromadb==0.4.15\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "wKxtrDBXviRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pzhuY_utvRdz"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "# URL of the Wikipedia page to scrape\n",
        "url = 'https://en.wikipedia.org/wiki/Prime_Minister_of_the_United_Kingdom'\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML content using BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Find all the text on the page\n",
        "text = soup.get_text()\n",
        "text = text.replace('\\n', '')\n",
        "\n",
        "# Open a new file called 'output.txt' in write mode and store the file object in a variable\n",
        "with open('output.txt', 'w', encoding='utf-8') as file:\n",
        "    # Write the string to the file\n",
        "    file.write(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# load the document\n",
        "with open('./output.txt', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# define the text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap  = 100,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "texts = text_splitter.create_documents([text])"
      ],
      "metadata": {
        "id": "JLEFXcwIvcHc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"testapikey\"\n",
        "\n",
        "# define the embeddings model\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# use the text chunks and the embeddings model to fill our vector store\n",
        "db = Chroma.from_documents(texts, embeddings)"
      ],
      "metadata": {
        "id": "NFBZdF2nvtqN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0.7)\n",
        "\n",
        "# Test current llm set-up without adding RAG (Retrieval Augmentation Generation)\n",
        "print(llm(\"Who is the current Prime Minister of the United Kingdom?\"))"
      ],
      "metadata": {
        "id": "tW74xpmsQ_4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the prompt template\n",
        "template = \"\"\"\n",
        "You are a chat bot who loves to help people! Given the following context sections, answer the\n",
        "question using only the given context. If you are unsure and the answer is not\n",
        "explicitly written in the documentation, say \"Sorry, I don't know how to help with that.\" The current year is 2023.\n",
        "\n",
        "Context sections:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{users_question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"users_question\"])"
      ],
      "metadata": {
        "id": "sYfYQ9Mrv_6R"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok==4.1.1\n",
        "!pip install flask_ngrok\n",
        "!ngrok authtoken 'testauthtoken'"
      ],
      "metadata": {
        "id": "vl7OQW4NQSzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, render_template, request\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)  # Start ngrok when app is run\n",
        "\n",
        "class Chatbot():\n",
        "  def __init__(self, llm_model, prompt_template, vector_store):\n",
        "        self.llm = llm_model\n",
        "        self.prompt = prompt_template\n",
        "        self.vector_store = vector_store\n",
        "\n",
        "  def generate_response(self, user_input):\n",
        "        if user_input.lower() == 'exit':\n",
        "            return \"Exiting the chatbot. Goodbye!\"\n",
        "\n",
        "        users_question = user_input\n",
        "\n",
        "        results = self.vector_store.similarity_search(query=users_question, n_results=5)\n",
        "\n",
        "        # Prepare prompt with user's question and retrieved context\n",
        "        prompt_text = self.prompt.format(context=results, users_question=users_question)\n",
        "\n",
        "        # Ask the defined language model based on the context and user's question\n",
        "        response = self.llm(prompt_text)\n",
        "\n",
        "        return response\n",
        "\n",
        "#Define a route for the home page\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return  render_template(\"index.html\")\n",
        "\n",
        "#Define a route for handling user input and getting the bot's response\n",
        "@app.route(\"/get\")\n",
        "def get_bot_response():\n",
        "    chatbot_object = Chatbot(llm_model=llm, prompt_template=prompt, vector_store=db)\n",
        "    user_input = request.args.get('msg')\n",
        "    return str(chatbot_object.generate_response(user_input))\n",
        "\n",
        "#Start the Flask application if this script is executed\n",
        "if __name__ == \"__main__\":\n",
        "    app.run() #debug=True"
      ],
      "metadata": {
        "id": "sHBIMGBLQTJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2hmNE8Govzxy"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}
